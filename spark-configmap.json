{
    "kind": "ConfigMap",
    "apiVersion": "v1",
    "metadata": {
        "name": "spark-apps",
        "namespace": "ecommerce-platform",
        "creationTimestamp": null
    },
    "data": {
        "batch_etl_k8s.py": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\n# Define configuration settings for Kubernetes deployment\n# These settings connect Spark to our Kubernetes services: Nessie for the catalog and MinIO for storage.\nCATALOG_NAME = \"nessie\"\nNESSIE_URI = \"http://nessie.ecommerce-platform.svc.cluster.local:19120/api/v1\"\nWAREHOUSE_PATH = \"s3a://warehouse\" # 'warehouse' is the bucket name in MinIO\nMINIO_ENDPOINT = \"http://minio.ecommerce-platform.svc.cluster.local:9000\"\n\n# Input data paths now point directly to MinIO (uploaded by submit-sample-jobs.sh)\n# These files are staged in the 'warehouse' bucket under data/ by the job submission script.\nUSERS_DATA_PATH = \"s3a://warehouse/data/users.csv\"\nPRODUCTS_DATA_PATH = \"s3a://warehouse/data/products.csv\"\n\ndef main():\n    \"\"\"\n    Main ETL job function for Kubernetes deployment.\n    Initializes Spark, reads CSV data, and writes it to Iceberg tables.\n    \"\"\"\n    spark = None\n    try:\n        # 1. Initialize Spark Session with Iceberg and S3 configurations\n        # =================================================================\n        print(\"Initializing Spark session for Kubernetes deployment...\")\n        \n        builder = (\n            SparkSession.builder\n            .appName(\"EcommercePlatform-BatchETL-K8s\")\n            .config(f\"spark.sql.catalog.{CATALOG_NAME}\", \"org.apache.iceberg.spark.SparkCatalog\")\n            .config(f\"spark.sql.catalog.{CATALOG_NAME}.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n            .config(f\"spark.sql.catalog.{CATALOG_NAME}.uri\", NESSIE_URI)\n            .config(f\"spark.sql.catalog.{CATALOG_NAME}.ref\", \"main\")\n            .config(f\"spark.sql.catalog.{CATALOG_NAME}.authentication.type\", \"NONE\")\n            .config(f\"spark.sql.catalog.{CATALOG_NAME}.warehouse\", WAREHOUSE_PATH)\n            .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\")\n            # S3/MinIO configuration for Kubernetes\n            .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n            .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n            .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n            .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n            # Additional configurations for better Kubernetes compatibility\n            .config(\"spark.sql.adaptive.enabled\", \"true\")\n            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n        )\n\n        spark = builder.getOrCreate()\n        print(\"Spark session initialized successfully for Kubernetes.\")\n\n        # 2. Create the 'demo' namespace (database) if it doesn't exist\n        # =================================================================\n        print(\"Creating database 'demo' if it does not exist...\")\n        spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {CATALOG_NAME}.demo\")\n        print(\"Database 'demo' is ready.\")\n\n        # 3. Process and Write the Users Table\n        # =================================================================\n        print(f\"Reading user data from {USERS_DATA_PATH}...\")\n        # For production, defining a schema explicitly is better than inferSchema.\n        users_df = spark.read.csv(USERS_DATA_PATH, header=True, inferSchema=True)\n        \n        # Simple transformation: Ensure signup_date is a date type\n        users_df = users_df.withColumn(\"signup_date\", col(\"signup_date\").cast(\"date\"))\n        \n        print(f\"Writing {users_df.count()} users to Iceberg table '{CATALOG_NAME}.demo.users'...\")\n        users_df.writeTo(f\"{CATALOG_NAME}.demo.users\").createOrReplace()\n        print(\"Users table created successfully.\")\n\n        # 4. Process and Write the Products Table\n        # =================================================================\n        print(f\"Reading product data from {PRODUCTS_DATA_PATH}...\")\n        products_df = spark.read.csv(PRODUCTS_DATA_PATH, header=True, inferSchema=True)\n\n        # Simple transformation: Ensure price is a double type\n        products_df = products_df.withColumn(\"price\", col(\"price\").cast(\"double\"))\n\n        print(f\"Writing {products_df.count()} products to Iceberg table '{CATALOG_NAME}.demo.products'...\")\n        products_df.writeTo(f\"{CATALOG_NAME}.demo.products\").createOrReplace()\n        print(\"Products table created successfully.\")\n\n        print(\"\\\\nETL Job Finished Successfully!\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        raise\n    finally:\n        if spark:\n            print(\"Stopping Spark session.\")\n            spark.stop()\n\nif __name__ == '__main__':\n    main()"
    }
}
