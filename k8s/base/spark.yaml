apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-config
  namespace: ecommerce-platform
data:
  spark-defaults.conf: |
    spark.master                    spark://spark-master:7077
    spark.sql.warehouse.dir         /tmp/spark-warehouse
    spark.sql.adaptive.enabled     true
    spark.sql.adaptive.coalescePartitions.enabled  true
    # Iceberg + Nessie integration jars (fetched automatically). Adjust versions as needed.
    # If these coordinates 404, run `spark-submit --packages` manually with versions available on Maven Central.
    # Added iceberg-nessie module (contains org.apache.iceberg.nessie.NessieCatalog) and S3 support jars
    # so Spark can talk to MinIO via s3a. The aws-java-sdk-bundle version is aligned with Flink jars you vendored.
    # Order is not critical, but keeping Iceberg -> Nessie -> extensions -> S3 deps groups for readability.
    spark.jars.packages            org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.iceberg:iceberg-nessie:1.5.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.79.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.665
    # Make extensions globally available (so jobs that don't set them programmatically still work)
    spark.sql.extensions           org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions
    # Fix executor-driver connectivity: use service DNS name instead of pod hostname so executors can connect back
    spark.driver.host               spark-master.ecommerce-platform.svc.cluster.local
    spark.driver.bindAddress        0.0.0.0
    spark.driver.port               7001
    spark.driver.blockManager.port  7002
    spark.executor.memory           500m
    spark.executor.cores            1
    spark.driver.memory             512m
    spark.driver.cores              1
    # S3/MinIO Configuration
    spark.hadoop.fs.s3a.endpoint                http://minio:9000
    spark.hadoop.fs.s3a.access.key              minioadmin
    spark.hadoop.fs.s3a.secret.key              minioadmin
    spark.hadoop.fs.s3a.path.style.access       true
    spark.hadoop.fs.s3a.connection.ssl.enabled  false
    spark.hadoop.fs.s3a.impl                    org.apache.hadoop.fs.s3a.S3AFileSystem
    spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
---
apiVersion: v1
kind: Service
metadata:
  name: spark-master
  namespace: ecommerce-platform
spec:
  selector:
    app: spark
    component: master
  ports:
    - name: web-ui
      port: 8080
      targetPort: 8080
    - name: spark
      port: 7077
      targetPort: 7077
    - name: driver-range
      port: 7001
      targetPort: 7001
      protocol: TCP
    - name: driver-port-2
      port: 7002
      targetPort: 7002
      protocol: TCP  
    - name: driver-port-3
      port: 7003
      targetPort: 7003
      protocol: TCP
    - name: driver-port-4
      port: 7004
      targetPort: 7004
      protocol: TCP
    - name: driver-port-5
      port: 7005
      targetPort: 7005
      protocol: TCP
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: spark-worker
  namespace: ecommerce-platform
spec:
  selector:
    app: spark
    component: worker
  ports:
    - name: web-ui
      port: 8081
      targetPort: 8081
  clusterIP: None  # Headless service for workers
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-master
  namespace: ecommerce-platform
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spark
      component: master
  template:
    metadata:
      labels:
        app: spark
        component: master
    spec:
      containers:
      - name: spark-master
        image: localhost:5001/custom-spark:latest
        env:
        - name: SPARK_MODE
          value: "master"
        - name: SPARK_MASTER_HOST
          value: "0.0.0.0"
        - name: SPARK_MASTER_PORT
          value: "7077"
        - name: SPARK_MASTER_WEBUI_PORT
          value: "8080"
        ports:
        - containerPort: 7077
          name: spark
        - containerPort: 8080
          name: web-ui
        volumeMounts:
        - name: spark-config
          mountPath: /opt/bitnami/spark/conf/spark-defaults.conf
          subPath: spark-defaults.conf
        - name: data
          mountPath: /data
        - name: apps
          mountPath: /apps
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 2Gi
        livenessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
      volumes:
      - name: spark-config
        configMap:
          name: spark-config
      - name: data
        emptyDir: {}
      - name: apps
        configMap:
          name: spark-apps
          defaultMode: 0755
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-worker
  namespace: ecommerce-platform
spec:
  replicas: 2
  selector:
    matchLabels:
      app: spark
      component: worker
  template:
    metadata:
      labels:
        app: spark
        component: worker
    spec:
      containers:
      - name: spark-worker
        image: localhost:5001/custom-spark:latest
        env:
        - name: SPARK_MODE
          value: "worker"
        - name: SPARK_MASTER_URL
          value: "spark://spark-master:7077"
        - name: SPARK_WORKER_CORES
          value: "1"
        - name: SPARK_WORKER_MEMORY
          value: "1g"
        - name: SPARK_WORKER_PORT
          value: "8881"
        - name: SPARK_WORKER_WEBUI_PORT
          value: "8081"
        ports:
        - containerPort: 8881
          name: spark
        - containerPort: 8081
          name: web-ui
        volumeMounts:
        - name: spark-config
          mountPath: /opt/bitnami/spark/conf/spark-defaults.conf
          subPath: spark-defaults.conf
        - name: data
          mountPath: /data
        - name: apps
          mountPath: /apps
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 2Gi
        livenessProbe:
          httpGet:
            path: /
            port: 8081
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /
            port: 8081
          initialDelaySeconds: 10
          periodSeconds: 10
      volumes:
      - name: spark-config
        configMap:
          name: spark-config
      - name: data
        emptyDir: {}
      - name: apps
        configMap:
          name: spark-apps
          defaultMode: 0755